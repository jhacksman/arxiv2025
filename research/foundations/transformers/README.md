# Transformer Architecture and Attention Mechanisms

## Papers

### Attention Is All You Need (2017)
- **Authors**: Vaswani et al.
- **arXiv**: [1706.03762](https://arxiv.org/abs/1706.03762)
- **Category**: Machine Learning, Natural Language Processing
- **Summary**: The seminal paper that introduced the Transformer architecture, eliminating recurrence and convolutions in favor of self-attention mechanisms. This work forms the foundation of modern NLP models.
- **Key Contributions**:
  - Multi-head attention mechanism
  - Positional encoding
  - Transformer architecture
  - Scaled dot-product attention

### Neural Machine Translation by Jointly Learning to Align and Translate (2014)
- **Authors**: Bahdanau, Cho, and Bengio
- **arXiv**: [1409.0473](https://arxiv.org/abs/1409.0473)
- **Category**: Machine Learning, Natural Language Processing
- **Summary**: Introduced the attention mechanism for neural machine translation, allowing models to learn soft alignments between source and target sequences.

## Implementation Resources
- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) - Harvard NLP's step-by-step tutorial
