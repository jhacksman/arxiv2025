# DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning

## Paper Information
- **Title**: DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning
- **Authors**: DeepSeek-AI, Daya Guo, Dejian Yang, et al.
- **File**: [deepseek_r1.pdf](deepseek_r1.pdf)
- **Published**: January 22, 2025
- **Category**: Language Models, Reasoning

## Summary
This paper introduces two models, DeepSeek-R1-Zero and DeepSeek-R1, focusing on incentivizing reasoning capabilities in large language models through reinforcement learning. The models are designed to be open-sourced for the research community.

## Key Contributions
- Novel approach to incentivizing reasoning in LLMs
- Introduction of DeepSeek-R1-Zero and DeepSeek-R1 models
- Open-source contribution to the research community
- Advancement in LLM reasoning capabilities

## Impact
The work represents a significant step forward in improving the reasoning capabilities of large language models, with potential applications in various AI tasks requiring complex reasoning.
